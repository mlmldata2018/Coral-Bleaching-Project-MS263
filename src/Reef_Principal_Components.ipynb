{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis of reef characteristics\n",
    "This notebook is devoted to a principal component analysis to see what parameters of reef environment and bleaching may be grouped, and then to see whether reefs will cluster in a way which allows useful subsets to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# This code section is largely a duplicate of the data input in the\n",
    "# Bleaching_Project_Data_Setup notebook, with fewer comments.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from mytools import principal_component as pc\n",
    "# Now read our data for reef cell locations.\n",
    "# A copy of the data is in this repository.  The reference copy is in\n",
    "# my Coral-Model-Data repository in the ProjectionsPaper directory.\n",
    "mat_data = sio.loadmat('../data/ESM2M_SSTR_JD.mat')\n",
    "#print(mat_data)\n",
    "\n",
    "try:\n",
    "    cells\n",
    "except NameError:\n",
    "    # do nothing, it's okay not to have cells yet\n",
    "    pass\n",
    "else:\n",
    "    # Clear the variable so we don't get carryover from earlier tests. \n",
    "    cells = cells.iloc[0:0]\n",
    "# Put the lat/lon columns directly into a data frame.  Note that they are stored\n",
    "# with longitude first in the incoming data. \n",
    "cells = pd.DataFrame(mat_data['ESM2M_reefs_JD'], columns=['Lon', 'Lat'])\n",
    "# Put in a random variable for testing\n",
    "# cells['Rand'] = np.random.uniform(low=-10, high=10, size=len(modelBleaching))\n",
    "\n",
    "cells['abs_lat'] = abs(cells['Lat'])\n",
    "\n",
    "# It makes more sense to have longitude have a break at 0 than at +-180, since the latter is in the midddle of a coral area.\n",
    "cells['Lon'] = cells['Lon']-180*(np.sign(cells['Lon'])-1)\n",
    "\n",
    "# Now add bleaching counts from a specific run.\n",
    "bleach_data = sio.loadmat('../data/HughesCompEvents_selV_rcp60E=1OA=1.mat')\n",
    "# Put the bleaching counts into a data frame.\n",
    "modelBleaching = pd.DataFrame(bleach_data['events80_2016'])\n",
    "modelBleaching.rename(columns={0: 'Events'}, inplace=True)\n",
    "# Be we really want this in the cells dataframe\n",
    "# cells['Events'] = modelBleaching['Events']\n",
    "\n",
    "print(cells.head())\n",
    "\n",
    "# Now get 1861-1950 SST mean and variance for each reef\n",
    "sst = mat_data['SSTR_2M26_JD']\n",
    "del mat_data  # this is big and not used again.\n",
    "sst_mean = np.mean(sst, axis=1)\n",
    "sst_var = np.var(sst, axis=1)\n",
    "cells['SST'] = sst_mean\n",
    "cells['variance'] = sst_var\n",
    "print(cells.head())\n",
    "all_names = list(cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "Does it make sense to include Events (the variable we want to predict) in this analysis, or not?\n",
    "How do we determine which variables are the best independent predictors of Events, rather than just of each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('PCA including signed and unsigned latitude:')\n",
    "[eigenval, eigenvec, pct_acct, loadings, sorted_names] = pc.pca(np.asmatrix(cells),\n",
    "     all_names, standardize=True, sort=True)\n",
    "print('Sorted names:\\n', sorted_names)\n",
    "print('Orig names:  \\n', all_names)\n",
    "print('Eigenvalues:\\n', eigenval)\n",
    "print('Eigenvectors:\\n', eigenvec)\n",
    "print('Variance accounted for by each component:\\n', pct_acct)\n",
    "print('Component loadings:\\n', loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "print('PCA including only unsigned latitude:')\n",
    "print('Note that variable names do NOT correspond to the columns below, which are based on PCs.')\n",
    "print('all names:', all_names)\n",
    "reduced_names = copy.deepcopy(all_names)\n",
    "print('red names:', reduced_names)\n",
    "reduced_names.remove('Lat')\n",
    "reduced_names.remove('Lon')\n",
    "[eigenval, eigenvec, pct_acct, loadings, sorted_names] = pc.pca(np.asmatrix(cells[reduced_names]),\n",
    "     reduced_names, standardize=True, sort=True)\n",
    "print('Names:\\n', sorted_names)\n",
    "print('Eigenvalues:\\n', eigenval)\n",
    "print('Eigenvectors:\\n', eigenvec)\n",
    "print('Variance accounted for by each component:\\n', pct_acct)\n",
    "print('Component loadings:\\n', loadings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now look at factor loadings for the reduced list\n",
    "# A = V (sqrt(Lambda))\n",
    "# V = eigenvectors = eigenvec\n",
    "# Lambda = eigenvalue matrix = eigenval\n",
    "A = np.matmul(eigenvec, np.diag(eigenval)**0.5)\n",
    "# A now contains the loadings for PC1 in the first row, and so on.\n",
    "A\n",
    "plt.figure()\n",
    "plt.plot(A[:, 0], A[:, 1], 'o')\n",
    "plt.xlim([-1, 1])\n",
    "plt.ylim([-1, 1])\n",
    "plt.xlabel('PC1 loading')\n",
    "plt.ylabel('PC2 loading')\n",
    "for i, txt in enumerate(sorted_names):\n",
    "    plt.text(A[i,0]+0.05,A[i,1],txt)\n",
    "sorted_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, **how is it** that Longitude has the highest variance explained (62.6%), but it has the lowest weighting in the PC1 loading axis?  Tom Connolly pointed out that longitude doesn't mean anything numerically, so it may not be suitable for this analysis, even though corals are sometimes segregated by longitude (e.g. Atlantic vs. Pacific)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now show all reefs transformed to PC1 and PC2\n",
    "# Convert each observation (reef) to a point in PC1, PC2 coordinates.\n",
    "# WARNING: data has not been sorted, eigen* have!\n",
    "data = np.asarray(cells[reduced_names])\n",
    "data_normalized = (data - np.mean(data, 0)) / np.std(data, 0, ddof=1)\n",
    "\n",
    "pc1 = eigenval[0]*np.sum(data_normalized * eigenvec[0], 1)\n",
    "pc2 = eigenval[1]*np.sum(data_normalized * eigenvec[1], 1)\n",
    "plt.figure()\n",
    "plt.scatter(pc1, pc2, c=cells['Lon'])\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Reef data transformed to PC1 and PC2');\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusions\n",
    "After eliminating variables which don't make sense in this analysis, there are so few that PCA isn't useful.  I'll switch to another notebook and just try scatterplots.  I'll do it for the world and for regional groups."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
