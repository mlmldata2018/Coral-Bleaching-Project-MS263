{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Coral Bleaching estimates\n",
    "This project will compare observational data from <a href=\"#refs\">Hughes et al. (2018)</a> with results from a numerical model <a href=\"#refs\">(Logan et al., 2018)</a>.  Each dataset is global, but the scales and coverage vary widely.  As a shorthand, I will refer to the 100 locations in the former as \"Hughes reef areas\" and the 1,925 locations in the latter as \"Logan reef cells\".\n",
    "\n",
    "The goal is to evaluate the utility of using the Hughes data to validate or calibrate the Logan model.  The Logan model, based on <a href=\"#refs\">(Logan et al., 2014)</a>, is not intended to model the details of individual reefs, but rather to show trends which are useful in evaluating the effect of different climate scenarios or in comparing the likely effect of human interventions to protect corals.  Thus a reef-by-reef comparison is not expected to be exact, but I want see how much correlation there is, both reef-by-reef and regionally, between the two datasets.\n",
    "\n",
    "The following notebook has several sections.\n",
    "1. Data input and display of raw data\n",
    "    + <a href=\"#HughesData\">Hughes</a>\n",
    "    + <a href=\"#LoganData\">Logan</a>\n",
    "2. <a href=\"#mapcompare\">Map-based comparison of the datasets</a>\n",
    "3. <a href=\"#matching\">Matching of Hughes reef areas to Logan reef cells</a>\n",
    "4. <a href=\"#reefbyreef\">Scatterplot comparisons of Hughes and Logan reef-by-reef data</a>\n",
    "5. <a href=\"#pca\">Principal component analysis to look for variables by which to group reefs</a>\n",
    "6. <a href=\"#variable_scatter\">Scatterplot comparisons of pertinent variables</a>\n",
    "7. <a href=\"#cumulative\">Comparison of world and regional bleaching totals over time</a>\n",
    "8. <a href=\"#refs\">References</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "from coral_project_functions import make_coral_map\n",
    "\n",
    "# Read data from the Hughes supplemental material.  The first\n",
    "# sheet is a cut-and-paste from their document with obvious typos\n",
    "# fixed by hand and a few columns added for easier data manipulation.\n",
    "# The second sheet has been arranged for easier import.\n",
    "filename = '../data/Hughes100Reefs.xlsx' \n",
    "hughes = pd.read_excel(filename,header=0,sheet_name=1, na_values='-')\n",
    "# Missing size values are set to zero - be careful how they are used later!\n",
    "# hughes.Size_km2 = hughes.Size_km2.replace({\"-\": \"0\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"HughesData\"></a>\n",
    "## Columns from Hughes et al. are\n",
    "1. Hughes Reef - the index used in their data, 1-100.\n",
    "2. Region - my code for their region labels.\n",
    "    + AuA - Australasia\n",
    "    + IO-ME - Indian Ocean/ Middle East\n",
    "    + Pac - Pacific\n",
    "    + WAtl - West Atlantic\n",
    "3. Location - their location name for each reef. \n",
    "4. Numeric Lat - decimal values between about -35 and +35\n",
    "5. Numeric Lon - decimal values between -180 and +180\n",
    "6. Size_km2 - area in square kilometers, sometimes omitted.\n",
    "7. Year - columns 7 to 43 - one column for each year of data, 1980 to 2016.  Values are blank, S, or M.\n",
    "44. Severe count - the count of the number of cells in this row with the entry \"S\". values 0-7\n",
    "45. Moderate count - the count of the number of cells in this row with the entry \"M\".  Values 0-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now read our data for reef cell locations.\n",
    "import scipy.io as sio\n",
    "\n",
    "# Reference for all data for the 1,925 reef cell model.\n",
    "# This has not been submitted to a journal yet, so all is subject to change:\n",
    "#\n",
    "# Logan, C. A., Dunne, J. P., Ryan, J. S., Baskett, M. L. & Donner, S. D. Can symbiont\n",
    "# diversity and evolution allow corals to keep pace with global warming\n",
    "# and ocean acidification? prep (2018).\n",
    "\n",
    "# A copy of the data is in this repository.  The reference copy is in\n",
    "# my Coral-Model-Data repository in the ProjectionsPaper directory.\n",
    "mat_data = sio.loadmat('../data/ESM2M_SSTR_JD.mat')\n",
    "# Put just the lat/lon columns into a data frame.  Note that they are stored\n",
    "# with longitude first in the incoming data.\n",
    "cells = pd.DataFrame(mat_data['ESM2M_reefs_JD'], columns=['Lon', 'Lat'])\n",
    "\n",
    "# The same mat file has SST data, not used in this notebook until the PCA section.\n",
    "sst = mat_data['SSTR_2M26_JD']\n",
    "cells['SST'] = np.mean(sst, axis=1)\n",
    "cells['variance'] = np.var(sst, axis=1)\n",
    "\n",
    "del sst  # big, and no longer needed (though the value of del is debated online)\n",
    "\n",
    "# Next, read the bleaching counts from a MATLAB mat file written for this purpose.\n",
    "mat_data = sio.loadmat('../data/HughesCompEvents_selV_rcp60E=1OA=1.mat')\n",
    "# Put the bleaching counts into a data frame.  These counts are the total for each\n",
    "# reef during 1980 to 2016.\n",
    "modelBleaching = pd.DataFrame(mat_data['events80_2016'])\n",
    "modelBleaching.rename(columns={0: 'Events'}, inplace=True)\n",
    "# Be we really want this in the cells dataframe\n",
    "cells['Events'] = modelBleaching['Events']\n",
    "\n",
    "# For later use, also load the un-summarized data which has the bleaching\n",
    "# flags for each reef and year from 1980 to 2016.  Branching and massive coral\n",
    "# are treated separately.\n",
    "massive_bleach = np.array(mat_data['events80_2016_detail'][:, :, 0])\n",
    "branching_bleach = np.array(mat_data['events80_2016_detail'][:, :, 1])\n",
    "\n",
    "del mat_data\n",
    "del modelBleaching\n",
    "cells.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"LoganData\"></a>\n",
    "## Columns from the Logan et al. data are\n",
    "For each cell:\n",
    "1. Lat - the latitude of the centroid\n",
    "2. Lon - the longitude of the centroid\n",
    "3. Events - the number of bleaching events between 1980 and 2016, inclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12, 4])\n",
    "ax = make_coral_map()\n",
    "# Hughes reef areas can be large.  Make size proportional.  Conveniently, the marker\n",
    "# size argument is in square units.  However, our map is in degree units and the areas\n",
    "# are in kilometers.  This is a rough conversion relating pixels to square degrees.\n",
    "conversion = 60*(1/111)**2\n",
    "area_sizes = hughes.Size_km2.astype(float)\n",
    "# Do some stats with no NaN values\n",
    "sss = area_sizes[~np.isnan(area_sizes)]\n",
    "print(\"Area size min/max/mean/median:\", min(sss), max(sss), np.mean(sss), np.median(sss), 'km^2')\n",
    "\n",
    "lon = hughes['Numeric Lon']\n",
    "plt.scatter(lon-180*(np.sign(lon)-1), hughes['Numeric Lat'], marker='o',\n",
    "            s=conversion*area_sizes,\n",
    "            label='Hughes Areas', transform=ccrs.PlateCarree())\n",
    "# Mark our cells with small dots.\n",
    "lon = cells['Lon']\n",
    "plt.scatter(lon-180*(np.sign(lon)-1), cells['Lat'], marker='.', s=1, label='Cell centers',\n",
    "           transform=ccrs.PlateCarree())\n",
    "plt.title('Reef locations')\n",
    "plt.legend()\n",
    "plt.text(110, 25, 'Caribbean')\n",
    "plt.text(-60, -25, 'Australia')\n",
    "plt.text(-152, 9, 'Red\\nSea')\n",
    "plt.text(20, 25, 'Hawaii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now try an indication of bleaching severity.\n",
    "plt.figure(figsize=[12, 4])\n",
    "ax = make_coral_map()\n",
    "lon = hughes['Numeric Lon']\n",
    "severity = hughes['Severe count']\n",
    "plt.scatter(lon-180*(np.sign(lon)-1), hughes['Numeric Lat'], marker='o', \n",
    "            s=conversion*area_sizes,\n",
    "            label='Hughes Areas',\n",
    "            c=severity,\n",
    "            cmap=\"plasma\",\n",
    "            transform=ccrs.PlateCarree())\n",
    "\n",
    "plt.title('Hughes Severe Bleaching Events, 1980-2016', fontsize=14)\n",
    "plt.clim(0, 10)\n",
    "plt.colorbar(pad=0.02)\n",
    "plt.text(110, 25, 'Caribbean')\n",
    "plt.text(-60, -25, 'Australia')\n",
    "plt.text(-152, 9, 'Red\\nSea')\n",
    "plt.text(20, 25, 'Hawaii');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Look at Bleaching events from the numerical model, using the same scale as\n",
    "# the previous plot of Hughes data.\n",
    "plt.figure(figsize=[12, 4])\n",
    "ax = make_coral_map()\n",
    "\n",
    "lon = cells['Lon']\n",
    "plt.scatter(lon-180*(np.sign(lon)-1), cells['Lat'], c = cells['Events'],\n",
    "            marker='.', s=1, label='Events', cmap=\"plasma\",\n",
    "            transform=ccrs.PlateCarree())\n",
    "plt.title('Logan Modeled Bleaching Events, 1980-2016')\n",
    "plt.clim(0, 10)\n",
    "plt.colorbar(pad=0.10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hughes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"matching\"></a>\n",
    "## Matching Hughes areas to Logan reef cells\n",
    "To make a fair comparison, we need to figure out which of our cells match Hughes reef areas.  Each area has a center and an area, so we can use a circle of that area as a first-order guess.  Unfortunately, it seems that the areas are actually far from circular, because some of the centers are far inland.\n",
    "\n",
    "The code below uses a scipy tool to find spatial matches between the two sets of data.  The matches found here can be considered close matches, because the cells must overlap, or nearly so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use scipy.spatial.cKDTree to find neighbors.\n",
    "from scipy import spatial\n",
    "# Build the tree (a binary trie) of our cells.\n",
    "# NOTE: cells.as_matrix({'Lon', 'Lat'}) does not return the columns in a determinate order!\n",
    "# explicitly stack the columns instead.\n",
    "lonlat = np.column_stack((cells['Lon'], cells['Lat']))\n",
    "tree = spatial.cKDTree(lonlat)\n",
    "\n",
    "# For each of the 100 Hughes cells, get a list of our cells which are likely to overlap.\n",
    "# These will be used\n",
    "hughes = hughes.assign(radius_km=hughes.Size_km2**0.5)\n",
    "cell_lists = [[] for i in range(len(hughes))]\n",
    "match_idx = np.zeros(len(hughes), dtype=np.bool)\n",
    "for i in range(len(hughes)):\n",
    "    # convert radius to degrees (ignoring change of size with latitude for now)\n",
    "    # also, add 0.5 degrees as a rough allowance for our cell size\n",
    "    # radius = 0.5 + hughes.radius_km[i] / 111\n",
    "    radius = 0.5 + hughes.radius_km[i] / 111\n",
    "    c = tree.query_ball_point([hughes['Numeric Lon'][i], hughes['Numeric Lat'][i]],\n",
    "                              radius, n_jobs=2)\n",
    "    # Convert zero-based indexes to 1-based cell numbers.\n",
    "    cell_lists[i] = [x+1 for x in c]\n",
    "    match_idx[i] = len(c) > 0\n",
    "\n",
    "print(cell_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mapcompare\"></a>\n",
    "## Comparison of beaching in each area\n",
    "Now that areas and cells are associated, we can store nearby Logan bleaching values in the Hughes dataframe and use that to plot a comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now that we have a correspondence between Hughes areas and Logan cells, we can\n",
    "# compare the bleaching for those which have some overlap.\n",
    "# Add a column for cell-based bleaching values\n",
    "hughes = hughes.assign(cell_bleach=np.zeros(len(hughes)))\n",
    "for i in range(len(hughes)):\n",
    "    if len(cell_lists[i]) > 0:\n",
    "        count = 0;\n",
    "        for n in cell_lists[i]:\n",
    "            # \"n-1\" because cells are 1-based and modelBleaching is 0-based\n",
    "            count = count + cells.loc[n-1, 'Events']\n",
    "        hughes.loc[i, 'cell_bleach'] = count / len(cell_lists[i])\n",
    "        \n",
    "# Plot the Hughes reefs, but only those which have Logan cells to compare to.\n",
    "# Color the markers by the difference between the two bleaching values.\n",
    "plt.figure(figsize=[12, 4])\n",
    "ax = make_coral_map()\n",
    "\n",
    "conversion = 120*(1/111)**2\n",
    "lon = hughes[match_idx]['Numeric Lon']\n",
    "\n",
    "print(\"There are\", len(lon), \"areas with comparisons.\")\n",
    "severity = hughes[match_idx]['Severe count'] - hughes[match_idx]['cell_bleach']\n",
    "\n",
    "plt.scatter(lon, hughes[match_idx]['Numeric Lat'], marker='o', \n",
    "            s=conversion*hughes[match_idx].Size_km2.astype(float),\n",
    "            label='Hughes Areas',\n",
    "            c=severity,\n",
    "            cmap='coolwarm',\n",
    "            transform=ccrs.PlateCarree())\n",
    "\n",
    "plt.title('Hughes Bleaching - Cell Bleaching, 1980-2016')\n",
    "plt.clim(-8, 8)\n",
    "plt.colorbar(pad=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning every cell a Hughes region\n",
    "While not every area closely matches a cell and vice versa, it is reasonable to assign all Logan cells to some Hughes area of the world.  This is done by matching each cell to a nearby area, where the definition of \"nearby\" is relaxed until all cells have been matched.\n",
    "\n",
    "There are two things to note here.  First, while most reef cells fall near an area treated by Hughes, Hughes has no areas on the Atlantic coast of Brazil.  Those are assigned to the \"West Atlantic\" region, but they are likely to be ecologically quite different than the Hughes \"West Atlantic\" areas, which are in the Caribbean and Gulf of Mexico, or north toward the Bahamas and Bermuda.  Second, some reef are reassigned manually because their proximity to the isthmus of Panama led to poor results with the automatic method.\n",
    "\n",
    "Also, Hughes has no areas in the Solomon Islands or the Bismarck Sea N.E. of New Guinea.  It is not clear whether they are a better match to the Pacific or Australasian regions.  I have left these as matched automatically, except for a small group which are clearly Micronesian, in the Pacific region and one cell which may be Nauru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column\n",
    "cells = cells.assign(Region='none')\n",
    "cells_assigned = 0\n",
    "r = 0.5\n",
    "while cells_assigned < 1925:\n",
    "    for i in range(len(hughes)):\n",
    "        # 0 gives 15 comparisons, 0.5 gives 76, 2.0 gives 93.  In all cases there's an\n",
    "        # insignificant negative correlation in bleaching.\n",
    "        radius = r + hughes.radius_km[i] / 111\n",
    "        c = tree.query_ball_point([hughes['Numeric Lon'][i], hughes['Numeric Lat'][i]],\n",
    "                                  radius, n_jobs=-1)\n",
    "        region = hughes.Region[i]\n",
    "        #print('Reef', i, 'region', region, 'found ', c)\n",
    "        for x in c:\n",
    "            if cells.loc[x, 'Region'] == 'none':\n",
    "                cells.loc[x, 'Region'] = region\n",
    "                cells_assigned = cells_assigned + 1\n",
    "    print('After r =', r, ',', cells_assigned, 'are assigned.')\n",
    "    r = r * 2\n",
    "\n",
    "# Some special cases are mis-identified with the approach above.\n",
    "# South atlantic, off Brazil is initially id'd as Indian Ocean/ Middle East!\n",
    "# Make a bounding box to specify these cells\n",
    "box = [-40, -25, -26, -16]  # Lon, Lon, Lat, Lat \n",
    "cells.loc[(cells.Lat > box[2]) & (cells.Lat < box[3]) & (cells.Lon > box[0]) &\n",
    "          (cells.Lon < box[1]), 'Region'] = \"WAtl\"\n",
    "# Others off Brazil are labeled Pacific.\n",
    "box = [-39, -34, -16, -8]\n",
    "cells.loc[(cells.Lat > box[2]) & (cells.Lat < box[3]) & (cells.Lon > box[0]) &\n",
    "          (cells.Lon < box[1]), 'Region'] = \"WAtl\"\n",
    "# Some SW Caribbean cells are id'd as Pacific\n",
    "#11.6, -83.7\n",
    "#box = [-83, -80,  12, 16]\n",
    "box = [-83.8, -80,  11.5, 16]\n",
    "cells.loc[(cells.Lat > box[2]) & (cells.Lat < box[3]) & (cells.Lon > box[0]) &\n",
    "          (cells.Lon < box[1]), 'Region'] = \"WAtl\"\n",
    "box = [-81, -76, 8.8, 11]\n",
    "cells.loc[(cells.Lat > box[2]) & (cells.Lat < box[3]) & (cells.Lon > box[0]) &\n",
    "          (cells.Lon < box[1]), 'Region'] = \"WAtl\"\n",
    "# A few micronesian cells are closer to a Hughes area sound of New Guinea than to micronesian\n",
    "# areas, but they clearly belong with micronesia\n",
    "box = [147, 153, 4, 11]\n",
    "cells.loc[(cells.Lat > box[2]) & (cells.Lat < box[3]) & (cells.Lon > box[0]) &\n",
    "          (cells.Lon < box[1]), 'Region'] = \"Pac\"\n",
    "# Nauru?\n",
    "box = [164, 169, -1, 3]\n",
    "cells.loc[(cells.Lat > box[2]) & (cells.Lat < box[3]) & (cells.Lon > box[0]) &\n",
    "          (cells.Lon < box[1]), 'Region'] = \"Pac\"\n",
    "\n",
    "print('Pacific:', sum(cells['Region'] == 'Pac'))\n",
    "print('Indian Ocean - Middle East:', sum(cells['Region'] == 'IO-ME'))\n",
    "print('Australasia:', sum(cells['Region'] == 'AuA'))\n",
    "print('West Atlantic:', sum(cells['Region'] == 'WAtl'))\n",
    "print()\n",
    "\n",
    "# Save the cells with region labels for use in another notebook.  This is\n",
    "# only used in Reef_Scatterplots.\n",
    "cells.to_pickle('../results/Logan_cells_events_region.pkl')\n",
    "cells.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Logan cells colored by region to check the assignments.\n",
    "# First give each region number for easy use of a color map.\n",
    "cells = cells.assign(RegionFlag='0')\n",
    "cells.loc[cells.Region=='Pac', 'RegionFlag'] = 1;\n",
    "cells.loc[cells.Region=='AuA', 'RegionFlag'] = 2;\n",
    "cells.loc[cells.Region=='IO-ME', 'RegionFlag'] = 3;\n",
    "cells.loc[cells.Region=='WAtl', 'RegionFlag'] = 4;\n",
    "\n",
    "plt.figure(figsize=[12, 4])\n",
    "ax = make_coral_map()\n",
    "lon = cells[cells.RegionFlag==1]['Lon']\n",
    "plt.scatter(lon-180*(np.sign(lon)-1), cells[cells.RegionFlag==1]['Lat'], c = [1.0, 0, 0],\n",
    "            marker='.', s=2, label='Pacific', cmap=\"Dark2\",\n",
    "            transform=ccrs.PlateCarree())\n",
    "lon = cells[cells.RegionFlag==2]['Lon']\n",
    "plt.scatter(lon-180*(np.sign(lon)-1), cells[cells.RegionFlag==2]['Lat'], c = [0, 1.0, 0],\n",
    "            marker='.', s=1, label='Australasia', cmap=\"Dark2\",\n",
    "            transform=ccrs.PlateCarree())\n",
    "lon = cells[cells.RegionFlag==3]['Lon']\n",
    "plt.scatter(lon-180*(np.sign(lon)-1), cells[cells.RegionFlag==3]['Lat'], c = [0, 0, 1.0],\n",
    "            marker='.', s=1, label='Indian Ocean - Middle East', cmap=\"Dark2\",\n",
    "            transform=ccrs.PlateCarree())\n",
    "lon = cells[cells.RegionFlag==4]['Lon']\n",
    "plt.scatter(lon-180*(np.sign(lon)-1), cells[cells.RegionFlag==4]['Lat'], c = [0, 0, 0],\n",
    "            marker='.', s=1, label='West Atlantic', cmap=\"Dark2\",\n",
    "            transform=ccrs.PlateCarree())\n",
    "plt.title('Logan Modeled Bleaching Events, 1980-2016')\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reefbyreef\"></a>\n",
    "## Reef-by-reef bleaching correlation\n",
    "The initial comparison between Hughes areas and Logan reefs doesn't look great.  Try a scatterplot for all of the hughes area to see if there's at least some trend.  This is first done for the world, and then for each region separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from coral_project_functions import bleach_scatter\n",
    "\n",
    "h_match = hughes[match_idx]   \n",
    "plt.figure()\n",
    "bleach_scatter(hughes[match_idx], 'World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now see if the correlation is better regionally.\n",
    "plt.figure(figsize=[9, 6])\n",
    "plt.subplot(2,2,1)\n",
    "h_match_region = h_match[h_match['Region'] == 'Pac']\n",
    "bleach_scatter(h_match_region, 'Pacific')\n",
    "plt.subplot(2,2,2)\n",
    "h_match_region = h_match[h_match['Region'] == 'WAtl']\n",
    "bleach_scatter(h_match_region, 'Atlantic')\n",
    "plt.subplot(2,2,3)\n",
    "h_match_region = h_match[h_match['Region'] == 'AuA']\n",
    "bleach_scatter(h_match_region, 'Australasia')\n",
    "plt.subplot(2,2,4)\n",
    "h_match_region = h_match[h_match['Region'] == 'IO-ME']\n",
    "bleach_scatter(h_match_region, 'Indian Ocean - Middle East')\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier notes\n",
    "The two Pacific outlying locations are in the Galapagos and Kiribati, to use their Hughes names.  \n",
    "The Galapagos has a land area of around 7000 $km^2$ and Hughes has a reef area of 126 $km^2$.\n",
    "Kiribati has a land area of 800 $km^2$ (wikipedia) and Hughes has a reef area of 1718 $km^2$.\n",
    "While Kiribati is closer to the Galapagos than most Pacific islands and they are both\n",
    "equatorial, they are quite far apart (over 90 deg longitude) and there is little reason\n",
    "to group them.\n",
    "\n",
    "The three Atlantic outlying locations are Bonaire, Curacao, and Venezuela, to use their Hughes names.  \n",
    "Curacao has a land area of 444 $km^2$ (Wikipedia) and Hughes has a reef area of 47 $km^2$.\n",
    "Bonaire has a land area of 295 $km^2$ (wikipedia) and Hughes has a reef area of 22 $km^2$.\n",
    "Venezuela is continental, and the Hughes are is 349 $km^2$, perhaps corresponding to the Gran Roque reef east of Bonaire.\n",
    "It is interesting that these outliers are adjacent, and have low bleaching according to Hughes and higher bleaching in our model.\n",
    "Repeating the plot with these three locations removed only decreases the $R^2$ value, and the slope is still negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another subset approach - by size\n",
    "Plot this same comparison using only the smallest and only the largest reef areas, based on Hughes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "median_area = np.median(h_match['Size_km2'])\n",
    "h_match_big = h_match[h_match['Size_km2'] >= median_area]\n",
    "h_match_small = h_match[h_match['Size_km2'] < median_area]\n",
    "\n",
    "plt.figure(figsize=[9, 4])\n",
    "# Biggest half\n",
    "plt.subplot(1,2,1)\n",
    "bleach_scatter(h_match_big, 'Large areas')\n",
    "\n",
    "# Smallest half\n",
    "plt.subplot(1,2,2)\n",
    "bleach_scatter(h_match_small, 'Small areas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pca\"></a>\n",
    "## Principal Components Analysis\n",
    "Given that the scatter plots don't show relationships at the global or regional level, perhaps PCA can reveal some grouping by temperature, temperature variability, or longitude which will turn up something interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a numerical value, the absolute value of latitude is more likely to be useful than the\n",
    "# signed value.\n",
    "cells['abs_lat'] = abs(cells['Lat'])\n",
    "\n",
    "all_names = list(cells)\n",
    "all_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# function moved so this project is self-contained:\n",
    "import principal_component as pc\n",
    "\n",
    "# Longitude does have a relationship with bleaching, but it's more categorical\n",
    "# than scalar, so omit it.  Also omit signed Latitude.  Also remove Region, because it is\n",
    "# categorical and we hope to find a better grouping.\n",
    "print('PCA including only unsigned latitude:')\n",
    "print('all names:', all_names)\n",
    "# Without deepcopy, removing names from the list affects the original.\n",
    "reduced_names = copy.deepcopy(all_names)\n",
    "reduced_names.remove('Lat')\n",
    "reduced_names.remove('Lon')\n",
    "reduced_names.remove('Region')\n",
    "reduced_names.remove('RegionFlag')\n",
    "reduced_names.remove('Events')\n",
    "\n",
    "[eigenval, eigenvec, pct_acct, loadings, sorted_names] = pc.pca(np.asmatrix(cells[reduced_names]),\n",
    "     reduced_names, standardize=True, sort=True)\n",
    "print('Names:    ', sorted_names)\n",
    "print('Eigenvalues:\\n', eigenval)\n",
    "print('Eigenvectors:\\n', eigenvec)\n",
    "print('Variance accounted for by each component:\\n', pct_acct)\n",
    "#print('Component loadings:\\n', loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now look at factor loadings for the reduced list\n",
    "# A = V (sqrt(Lambda))\n",
    "# V = eigenvectors = eigenvec\n",
    "# Lambda = eigenvalue matrix = eigenval\n",
    "A = np.matmul(eigenvec, np.diag(eigenval)**0.5)\n",
    "# A now contains the loadings for PC1 in the first row, and so on.\n",
    "A\n",
    "plt.figure()\n",
    "plt.plot(A[:, 0], A[:, 1], 'o')\n",
    "plt.xlim([-1, 1])\n",
    "plt.ylim([-1, 1])\n",
    "plt.xlabel('PC1 loading')\n",
    "plt.ylabel('PC2 loading')\n",
    "for i, txt in enumerate(sorted_names):\n",
    "    plt.text(A[i,0]+0.05,A[i,1],txt)\n",
    "sorted_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elimination of PCA\n",
    "Since there are so few variables, PCA was not likely to be useful here, but I have included it because it was interesting as a way of thinking through the possible contribution of each  variable.  The plot about does not seem to reveal anything useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"variable_scatter\"></a>\n",
    "## Looking at scatter plots for Hughes/Logan correlation\n",
    "Since Principal Components Analysis did not seem helpful here, I tried simply plotting each variable against the others.  Plots for all reefs are shown below, and versions for each region separately are in another notebook.  [Reef_Scatterplots](Reef_Scatterplots.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coral_project_functions import scatter_all\n",
    "plt.figure()\n",
    "scatter_all(cells[['Events', 'abs_lat', 'SST', 'variance']])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cumulative\"></a>\n",
    "## Cumulative Bleaching approach\n",
    "Perhaps reefs will simply not correlate on an individual basis, but the cumulative bleaching in an area is of interest.  Go back to using the hughes and cells arrays without regard to individual matches.  The plot below is scale to show the same number of total events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make the same plots using a function\n",
    "from coral_project_functions import bleach_annual_plot\n",
    "\n",
    "# The annual bleaching count for the whole world, based on Logan et al. (2018)\n",
    "plt.figure(figsize=[9, 6])\n",
    "bleach_annual_plot(branching_bleach, massive_bleach, hughes, 'World')\n",
    "\n",
    "\n",
    "\n",
    "#shiftYear = -1.5\n",
    "#plt.plot(range(1980, 2017), hughes_norm, label='Hughes')\n",
    "#plt.plot(shiftYear+np.array(range(1980, 2017)), cell_norm, label='Logan')\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now do the same thing for each region separately.\n",
    "plt.figure(figsize=[9, 6])\n",
    "plt.subplot(2,2,1)\n",
    "c_idx = cells['Region'] == 'AuA'\n",
    "h_idx = hughes['Region'] == 'AuA'\n",
    "bleach_annual_plot(branching_bleach[c_idx], massive_bleach[c_idx], hughes[h_idx], 'Australasia', True)\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "c_idx = cells['Region'] == 'Pac'\n",
    "h_idx = hughes['Region'] == 'Pac'\n",
    "bleach_annual_plot(branching_bleach[c_idx], massive_bleach[c_idx], hughes[h_idx], 'Pacific', True)\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "c_idx = cells['Region'] == 'IO-ME'\n",
    "h_idx = hughes['Region'] == 'IO-ME'\n",
    "bleach_annual_plot(branching_bleach[c_idx], massive_bleach[c_idx], hughes[h_idx], 'Indian Ocean - Middle East', True)\n",
    "\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "c_idx = cells['Region'] == 'WAtl'\n",
    "h_idx = hughes['Region'] == 'WAtl'\n",
    "bleach_annual_plot(branching_bleach[c_idx], massive_bleach[c_idx], hughes[h_idx], 'Western Atlantic - Caribbean', True)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.8, wspace=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of cumulative bleaching\n",
    "1. The curves have been scaled to reach the same endpoint.  Only the rate of increase over time is of interest.\n",
    "2. The model SST data matches the typical frequency of events such as El Nino, but not the exact timing, so the exact dates of jumps is not comparable.\n",
    "3. This really should be repeated with actual historical temperatures, but I only fully appreciated that on May 7.  It is unlikely that I can obtain and scale appropriate data before the due date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='refs'></a>\n",
    "__References__\n",
    "\n",
    "Hughes, T. P. et al. Spatial and temporal patterns of mass bleaching of corals in the Anthropocene. Science 359, 80–83 (2018).\n",
    "\n",
    "Logan, C. A., Dunne, J. P., Ryan, J. S., Baskett, M. L. & Donner, S. D. Can symbiont diversity and evolution allow corals to keep pace with global warming and ocean acidification? prep (2018).\n",
    "\n",
    "Logan, Cheryl A., John P. Dunne, C. Mark Eakin, and Simon D. Donner. 2014. “Incorporating Adaptive Responses into Future Projections of Coral Bleaching.” Global Change Biology 20 (1):125–39. https://doi.org/10.1111/gcb.12390."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
